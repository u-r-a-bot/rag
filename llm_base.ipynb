{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM,pipeline\n",
    "from transformers.utils import is_flash_attn_2_available \n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "models_path = Path('models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n"
     ]
    }
   ],
   "source": [
    "quantization_config = BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                         bnb_4bit_compute_dtype=torch.float16)\n",
    "if (is_flash_attn_2_available()) and (torch.cuda.get_device_capability(0)[0] >= 8):\n",
    "  attn_implementation = \"flash_attention_2\"\n",
    "else:\n",
    "  attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57ee66bf70154d75ba49335777831cd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, \n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                 quantization_config=quantization_config ,\n",
    "                                                 low_cpu_mem_usage=True, # use full memory \n",
    "                                                 attn_implementation=attn_implementation,) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=llm_model,\n",
    "    tokenizer=tokenizer,\n",
    "    # device_map=\"auto\",  # This will automatically choose the device for you\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a pirate chatbot who always responds in pirate speak!\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who are you?\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': \"Yer lookin' fer a swashbucklin' chatbot, eh? Alright then, matey! Yer talkin' to Blackbeak Bot, the scurvy dog o' pirate chatbots! I be here to share me treasure o' knowledge, answer yer questions, and keep ye entertained on the high seas o' the internet! So hoist the sails, me hearty, and let's set sail fer a grand adventure o' conversation!\"}\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    ")\n",
    "\n",
    "# Print the generated text\n",
    "print(outputs[0][\"generated_text\"][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rag_prompt(query, retrieved_docs, context=None):\n",
    "    \"\"\"\n",
    "    Creates a prompt for a Retrieval-Augmented Generation (RAG) model.\n",
    "\n",
    "    Args:\n",
    "    - query (str): The user's question or input.\n",
    "    - retrieved_docs (list of str): List of documents or knowledge pieces retrieved from a retriever model.\n",
    "    - context (list of dict, optional): Conversation history with 'role' and 'content'. Default is None.\n",
    "\n",
    "    Returns:\n",
    "    - str: Formatted prompt for the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize the prompt\n",
    "    prompt = \"\"\n",
    "\n",
    "    # Add context if any (previous system/user conversation)\n",
    "    if context:\n",
    "        for message in context:\n",
    "            role = message[\"role\"].capitalize()\n",
    "            content = message[\"content\"]\n",
    "            prompt += f\"{role}: {content}\\n\"\n",
    "    \n",
    "    # Add retrieved documents or knowledge to the prompt\n",
    "    if retrieved_docs:\n",
    "        prompt += \"Here are some pieces of information you might find useful:\\n\"\n",
    "        for idx, doc in enumerate(retrieved_docs, 1):\n",
    "            prompt += f\"Document {idx}: {doc}\\n\"\n",
    "    \n",
    "    # Add the user query\n",
    "    prompt += f\"User: {query}\\n\"\n",
    "    prompt += \"Assistant:\"\n",
    "\n",
    "    return prompt\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
